{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeWEr7SVACGP",
        "outputId": "527f9539-c6b8-4b88-dfb0-b764a5cb5dd2"
      },
      "outputs": [],
      "source": [
        "pip install trimesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzg3HwMO_5vD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import glob\n",
        "import trimesh\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import Conv3D, Dense ,Dropout, Flatten, Activation, MaxPooling3D, Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "nrAEYus-AVcE",
        "outputId": "4b50b0fa-7ebc-4dfd-a9ae-27bad11f04cc"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    print(\"reading from cloud...\")\n",
        "    path = \"/content/drive/MyDrive/Colab Notebooks/MPPE/stls_opt/\"\n",
        "except:\n",
        "    import pathlib\n",
        "    print(\"reading from disk\")\n",
        "    path = str(pathlib.Path().resolve())+\"/dataset/stls_opt/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "_-ueyYuw_5vI",
        "outputId": "838360c8-f478-4bc1-da2a-3ef4aeae52c8"
      },
      "outputs": [],
      "source": [
        "mesh = trimesh.load(path+\"63_8475_10313_17790_28845.stl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUaw_pNR_5vJ"
      },
      "outputs": [],
      "source": [
        "def augment(points):\n",
        "    # jitter points\n",
        "    points += tf.random.uniform(points.shape, -0.05, 0.05, dtype=tf.float32)\n",
        "    # shuffle points\n",
        "    points = tf.random.shuffle(points)\n",
        "    return points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mv =np.array(mesh.volume)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#a = np.zeros(0)\n",
        "a = np.append(a,mesh.volume+mesh.volume*2)\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoC-qUC9_5vK",
        "outputId": "e2814192-21d0-43ac-d112-baf8b2f4330d"
      },
      "outputs": [],
      "source": [
        "n_samples = 10000\n",
        "n_aug_samples = 1\n",
        "X_train = np.zeros([0,n_samples,3])\n",
        "X_train_volume = np.zeros(0)\n",
        "y_train = np.zeros(0)\n",
        "for filename in os.listdir(path):\n",
        "    if filename.endswith(\".stl\"):\n",
        "        print(\"appending\", filename)\n",
        "        mesh = trimesh.load(path+filename)\n",
        "        points = mesh.sample(n_samples)\n",
        "        \n",
        "        for n in range(n_aug_samples):\n",
        "\n",
        "            mv = mesh.volume\n",
        "            mv = mv + mv*random.randint(-5,5)/10000\n",
        "            X_train_volume = np.append(X_train_volume,mv)\n",
        "\n",
        "            points = augment(points)\n",
        "            X_train = np.append(X_train,points[np.newaxis,...],axis=0)\n",
        "            y_train = np.append(y_train,filename.split(\"_\")[3])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPD96k6r_5vM",
        "outputId": "c1bdb09b-b3f7-40a5-9b8e-c55060f70410"
      },
      "outputs": [],
      "source": [
        "points = mesh.sample(5000)\n",
        "#points = augment(points)\n",
        "for points in X_train:\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    ax = fig.add_subplot(111,projection=\"3d\")\n",
        "    ax.scatter(points[:, 0], points[:, 2], points[:, 1])\n",
        "    ax.set_axis_off()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_volume.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lifZBOwN_5vN",
        "outputId": "feb7f132-ecb8-4136-cbb6-42dcbc0cdc37"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5awA0CU_5vO",
        "outputId": "3944cfc5-7b23-4f61-acfb-e24138380676"
      },
      "outputs": [],
      "source": [
        "#Load model (LSTM_3D is the one that worked the best), set learning rate and number of classes (final layer length), then train the data (change epoch for more training cycles)\n",
        "#model = CNN_3D_REG(X_train,learning_rate= 0.00002)\n",
        "#print(model.summary())\n",
        "# model.fit(X_train, y_train, batch_size = 8, validation_split = 0.2, epochs = 15, verbose = 2)\n",
        "model.fit(X_train, y_train, batch_size = 8,validation_split = 0.2, epochs = 10, verbose = 2)\n",
        "#esults = model.evaluate(X_test, y_test, verbose = 2)\n",
        "print('Test accuracy: ', results[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_volume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coLrMl7Q_5vP"
      },
      "outputs": [],
      "source": [
        "input1 = keras.Input(shape=(X_train.shape[1:]))\n",
        "input2 = keras.Input(shape=(1))\n",
        "\n",
        "y = Dense(1)(input2)\n",
        "\n",
        "x = tnet(input1, 3)\n",
        "x = conv_bn(x, 32)\n",
        "x = conv_bn(x, 32)\n",
        "x = tnet(x, 32)\n",
        "x = conv_bn(x, 32)\n",
        "x = conv_bn(x, 64)\n",
        "x = conv_bn(x, 512)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = dense_bn(x, 256)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "x = dense_bn(x, 128)\n",
        "#x = layers.Dropout(0.3)(x)\n",
        "\n",
        "x = layers.Concatenate()([x,y])\n",
        "\n",
        "dense = Dense(2000, activation = 'relu')(x)\n",
        "dense = Dense(300, activation = 'relu')(dense)\n",
        "dense = Dense(150, activation = 'relu')(dense)\n",
        "dense = Dense(20, activation = 'relu')(dense)\n",
        "dense = Dense(16, activation = 'relu')(dense)\n",
        "    # final layer with 10 neurons to classify the instances\n",
        "output = Dense(1, activation = 'linear')(dense)\n",
        "    \n",
        "#outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=[input1,input2], outputs=output, name=\"pointnet\")\n",
        "#model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCCIm-ql_5vR"
      },
      "outputs": [],
      "source": [
        "def augment(points):\n",
        "    # jitter points\n",
        "    points += tf.random.uniform(points.shape, -0.005, 0.005, dtype=tf.float64)\n",
        "    # shuffle points\n",
        "    points = tf.random.shuffle(points)\n",
        "    return points\n",
        "\n",
        "#fuctions to build models \n",
        "def conv_bn(x, filters):\n",
        "    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\")(x)\n",
        "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "    return layers.Activation(\"relu\")(x)\n",
        "\n",
        "\n",
        "def dense_bn(x, filters):\n",
        "    x = layers.Dense(filters)(x)\n",
        "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
        "    return layers.Activation(\"relu\")(x)\n",
        "\n",
        "class OrthogonalRegularizer(keras.regularizers.Regularizer):\n",
        "    def __init__(self, num_features, l2reg=0.001):\n",
        "        self.num_features = num_features\n",
        "        self.l2reg = l2reg\n",
        "        self.eye = tf.eye(num_features)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
        "        xxt = tf.tensordot(x, x, axes=(2, 2))\n",
        "        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
        "        return tf.reduce_sum(self.l2reg * tf.square(xxt - self.eye))\n",
        "\n",
        "#function to create T-net layers\n",
        "def tnet(inputs, num_features):\n",
        "\n",
        "    # Initalise bias as the indentity matrix\n",
        "    bias = keras.initializers.Constant(np.eye(num_features).flatten())\n",
        "    reg = OrthogonalRegularizer(num_features)\n",
        "\n",
        "    x = conv_bn(inputs, 32)\n",
        "    x = conv_bn(x, 64)\n",
        "    x = conv_bn(x, 512)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = dense_bn(x, 256)\n",
        "    x = dense_bn(x, 128)\n",
        "    x = layers.Dense(\n",
        "        num_features * num_features,\n",
        "        kernel_initializer=\"zeros\",\n",
        "        bias_initializer=bias,\n",
        "        activity_regularizer=reg,\n",
        "    )(x)\n",
        "    feat_T = layers.Reshape((num_features, num_features))(x)\n",
        "    # Apply affine transformation to input features\n",
        "    \n",
        "    return layers.Dot(axes=(2, 1))([inputs, feat_T])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "765aPkAD_5vR",
        "outputId": "f07dca82-65e8-487b-fdc6-bf1eb3211581"
      },
      "outputs": [],
      "source": [
        "y_train = np.float64(y_train)\n",
        "y_train_max = y_train.max()\n",
        "model.compile(\n",
        "    loss=\"MSE\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.00001),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit([X_train, X_train_volume], y_train/y_train_max, batch_size = 8, epochs = 5, verbose = 2,validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SBZORUO_5vT"
      },
      "outputs": [],
      "source": [
        "y_train_max = y_train.max()\n",
        "preds = model.predict(X_train)#*y_train_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q__hxu6g_5vU",
        "outputId": "94ca5948-8c43-4a55-c512-2efa940a65c8"
      },
      "outputs": [],
      "source": [
        "preds.flatten()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mppd.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "50ffb7d2e1c20b3fce74969edadf992c5e2c62f97786e5c092ebf6cc72a542c6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit ('MPPE': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
